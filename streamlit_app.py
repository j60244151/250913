# streamlit_app.py
import os
import io
import json
import re
import urllib.request
from typing import Dict, Tuple, Optional

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="MBTI Ã— Climate Map",
    page_icon="ğŸ—ºï¸",
    layout="wide",
)

st.title("ğŸ—ºï¸ MBTI ìœ í˜• Ã— ê¸°í›„(ìœ„ë„) ìƒê´€ ì‹œê°í™”")
st.caption("ê°™ì€ í´ë”ì˜ CSVë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì½ê³ , ì—†ìœ¼ë©´ ì—…ë¡œë“œë¡œ ëŒ€ì²´ Â· Altair ì¸í„°ë™í‹°ë¸Œ ì›”ë“œë§µ Â· ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë¶ˆí•„ìš”")

# -----------------------------
# Constants
# -----------------------------
MBTI_TYPES = [
    "INTJ","INTP","ENTJ","ENTP",
    "INFJ","INFP","ENFJ","ENFP",
    "ISTJ","ISFJ","ESTJ","ESFJ",
    "ISTP","ISFP","ESTP","ESFP"
]
LOCAL_DATA_PATH = "countriesMBTI_16types.csv"

WORLD_TOPOJSON_URL = "https://cdn.jsdelivr.net/npm/world-atlas@2/countries-110m.json"
# Columns: CountryName, CapitalName, CapitalLatitude, CapitalLongitude, ContinentName, ...
CAPITALS_CSV_URL = "https://raw.githubusercontent.com/icyrockcom/country-capitals/master/data/country-list.csv"

NAME_FIX: Dict[str, str] = {
    "united states": "united states of america",
    "usa": "united states of america",
    "u s a": "united states of america",
    "russia": "russian federation",
    "south korea": "korea republic of",
    "north korea": "korea democratic people s republic of",
    "czech": "czechia",
    "czech republic": "czechia",
    "swaziland": "eswatini",
    "cape verde": "cabo verde",
    "laos": "lao people s democratic republic",
    "moldova": "moldova republic of",
    "ivory coast": "cote d ivoire",
    "brunei darussalam": "brunei",
    "vatican": "holy see",
}

# -----------------------------
# Utils
# -----------------------------
def norm_name(name: str) -> str:
    if not isinstance(name, str):
        return ""
    s = name.lower().strip()
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

def fix_name_for_join(s: str) -> str:
    s0 = norm_name(s)
    return NAME_FIX.get(s0, s0)

@st.cache_data(show_spinner=False)
def robust_read_csv(source) -> pd.DataFrame:
    """Try multiple encodings for path/UploadedFile/URL."""
    if isinstance(source, str) and os.path.exists(source):
        return pd.read_csv(source)
    if hasattr(source, "read"):  # Streamlit UploadedFile
        raw = source.read()
        for enc in ("utf-8","utf-8-sig","cp949","euc-kr","latin-1"):
            try:
                return pd.read_csv(io.BytesIO(raw), encoding=enc)
            except Exception:
                pass
        return pd.read_csv(io.BytesIO(raw), engine="python")
    return pd.read_csv(source)

@st.cache_data(show_spinner=False)
def fetch_json(url: str) -> dict:
    with urllib.request.urlopen(url) as resp:
        return json.loads(resp.read().decode("utf-8"))

@st.cache_data(show_spinner=False)
def fetch_csv(url: str) -> pd.DataFrame:
    return pd.read_csv(url)

def auto_detect_country_col(df: pd.DataFrame) -> Optional[str]:
    for cand in ["Country","country","êµ­ê°€","êµ­ê°€ëª…","Nation","Region","Country/Region"]:
        if cand in df.columns:
            return cand
    # fallback: ì²« ë²ˆì§¸ ë¹„ìˆ˜ì¹˜í˜• ì—´
    for c in df.columns:
        if not pd.api.types.is_numeric_dtype(df[c]):
            return c
    return None

def to_wide_by_country(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:
    """
    CSVë¥¼ êµ­ê°€ë³„ ê°€ë¡œí˜• í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ê³ ,
    íƒì§€í•œ êµ­ê°€ ì—´ì„ í•­ìƒ 'Country'ë¡œ ê°•ì œ í†µì¼í•˜ì—¬ ë°˜í™˜í•œë‹¤.
    """
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]

    # 1) êµ­ê°€ ì—´ íƒì§€ + 'Country'ë¡œ ê°•ì œ í†µì¼
    detected = auto_detect_country_col(df)
    if not detected:
        raise ValueError("êµ­ê°€ ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆ: Country)")
    if detected != "Country":
        df = df.rename(columns={detected: "Country"})
    country_col = "Country"

    # 2) MBTI ì—´ ì°¾ê¸°
    up_map = {c: c.upper().strip() for c in df.columns}
    mbti_cols_present = [orig for orig, up in up_map.items() if up in MBTI_TYPES]

    # 3) ê°€ë¡œí˜• ì²˜ë¦¬
    if len(mbti_cols_present) >= 4:
        wide = df[[country_col] + mbti_cols_present].copy()
        # MBTI ì—´ ì´ë¦„ ëŒ€ë¬¸ì í†µì¼
        rename_map = {c: up_map[c] for c in mbti_cols_present}
        wide = wide.rename(columns=rename_map)
        # 16ê°œ ëª¨ë‘ ë³´ì¥
        for t in MBTI_TYPES:
            if t not in wide.columns:
                wide[t] = np.nan
        # ìˆ«ìí™”
        for t in MBTI_TYPES:
            wide[t] = pd.to_numeric(wide[t], errors="coerce")
        # ì—´ ìˆœì„œ ê³ ì •
        wide = wide[[country_col] + MBTI_TYPES]
        return wide, country_col

    # 4) ì„¸ë¡œí˜• ì²˜ë¦¬
    lower = {c.lower(): c for c in df.columns}
    type_col = next((lower[c] for c in ["type","mbti","mbti_type","ìœ í˜•"] if c in lower), None)
    value_col = next((lower[c] for c in ["value","percent","percentage","ratio","ë¹„ìœ¨","í¼ì„¼íŠ¸","ê°’"] if c in lower), None)
    if type_col and value_col:
        long = df[[country_col, type_col, value_col]].copy()
        long.columns = [country_col, "Type", "Value"]
        long["Type"] = long["Type"].astype(str).str.upper().str.strip()
        long = long[long["Type"].isin(MBTI_TYPES)]
        long["Value"] = pd.to_numeric(long["Value"], errors="coerce")
        wide = long.pivot_table(index=country_col, columns="Type", values="Value", aggfunc="mean").reset_index()
        for t in MBTI_TYPES:
            if t not in wide.columns:
                wide[t] = np.nan
        wide = wide[[country_col] + MBTI_TYPES]
        return wide, country_col

    raise ValueError("MBTI 16ê°œ ì—´(ê°€ë¡œí˜•) ë˜ëŠ” (Type, Value) êµ¬ì„±(ì„¸ë¡œí˜•)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

def normalize_rows_to_100(wide: pd.DataFrame, country_col: str) -> pd.DataFrame:
    """í–‰ ê¸°ì¤€ í•©ê³„ 100%ë¡œ ì •ê·œí™” (ì›ë³¸ì´ 0~1 ë¹„ìœ¨ì´ì–´ë„ %ë¡œ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜)."""
    out = wide.copy()
    X = out[MBTI_TYPES].astype(float)
    row_sum = X.sum(axis=1, skipna=True).replace(0, np.nan)
    out[MBTI_TYPES] = (X.div(row_sum, axis=0) * 100.0)
    return out

def classify_climate(abs_lat: float) -> str:
    """ì•„ì£¼ ë‹¨ìˆœí•œ ìœ„ë„ ê¸°ë°˜ ê¸°í›„ëŒ€ ë¶„ë¥˜."""
    if pd.isna(abs_lat):
        return "Unknown"
    if abs_lat < 23.5:
        return "Tropical"
    elif abs_lat < 35:
        return "Subtropical"
    elif abs_lat < 60:
        return "Temperate"
    else:
        return "Polar"

# -----------------------------
# 1) ë°ì´í„° ì…ë ¥ (ë¡œì»¬ ìš°ì„ , ì—†ìœ¼ë©´ ì—…ë¡œë“œ)
# -----------------------------
st.sidebar.header("ğŸ“„ ë°ì´í„° ì…ë ¥")
mbti_df = None
source_label = None

if os.path.exists(LOCAL_DATA_PATH):
    try:
        mbti_df = robust_read_csv(LOCAL_DATA_PATH)
        source_label = f"ë¡œì»¬ íŒŒì¼ ì‚¬ìš©: `{LOCAL_DATA_PATH}`"
    except Exception as e:
        st.sidebar.warning(f"ë¡œì»¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

if mbti_df is None:
    up = st.sidebar.file_uploader("MBTI êµ­ê°€ ë°ì´í„° CSV ì—…ë¡œë“œ", type=["csv"])
    if up is not None:
        mbti_df = robust_read_csv(up)
        source_label = "ì—…ë¡œë“œ íŒŒì¼ ì‚¬ìš©"
    else:
        st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”. ì˜ˆì‹œ: countriesMBTI_16types.csv")
        st.stop()

st.sidebar.success(source_label)

# -----------------------------
# 2) êµ¬ì¡° í•´ì„ + ì •ê·œí™”
# -----------------------------
try:
    wide, country_col = to_wide_by_country(mbti_df)  # country_colì€ í•­ìƒ 'Country'
except Exception as e:
    st.error(f"ë°ì´í„° êµ¬ì¡° í•´ì„ ì‹¤íŒ¨: {e}")
    st.stop()

wide_norm = normalize_rows_to_100(wide, country_col)   # % ê¸°ì¤€
long_norm = wide_norm.melt(id_vars=[country_col], value_vars=MBTI_TYPES,
                           var_name="MBTI", value_name="Percent")

# -----------------------------
# 3) ìˆ˜ë„ ìœ„ë„/ëŒ€ë¥™ (ì—…ë¡œë“œ ì„ íƒ ê°€ëŠ¥)
# -----------------------------
st.sidebar.subheader("ğŸŒ ìˆ˜ë„ ìœ„ë„(ê¸°í›„) / ëŒ€ë¥™ ë°ì´í„°")
capitals_df = None
cap_up = st.sidebar.file_uploader("ìˆ˜ë„Â·ìœ„ë„ CSV(ì„ íƒ)", type=["csv"],
                                  help="ì—´ ì˜ˆì‹œ: CountryName, CapitalLatitude, CapitalLongitude, ContinentName")

try:
    if cap_up is not None:
        capitals_df = robust_read_csv(cap_up)
        st.sidebar.success("ì—…ë¡œë“œí•œ ìˆ˜ë„Â·ìœ„ë„ CSV ì‚¬ìš©")
    else:
        capitals_df = fetch_csv(CAPITALS_CSV_URL)
        st.sidebar.caption("ê¸°ë³¸ ê³µê°œ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš© ì¤‘ (country-capitals)")
except Exception as e:
    st.sidebar.warning(f"ìˆ˜ë„/ìœ„ë„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}. ê¸°í›„ êµ¬ë¶„ì€ Unknownìœ¼ë¡œ í‘œê¸°ë©ë‹ˆë‹¤.")

# ì´ë¦„ ì •ê·œí™”í•˜ì—¬ ì¡°ì¸ í‚¤ ìƒì„±
wide_norm["_join"] = wide_norm[country_col].apply(fix_name_for_join)

if capitals_df is not None:
    caps = capitals_df.rename(columns={
        "CountryName":"CountryName",
        "CapitalLatitude":"CapitalLatitude",
        "CapitalLongitude":"CapitalLongitude",
        "ContinentName":"ContinentName"
    })
    # ìœ ì—°í•œ í—¤ë” ë§¤í•‘
    alias = {
        "CountryName": ["CountryName","Country","Name","Country Name"],
        "CapitalLatitude": ["CapitalLatitude","Latitude","Capital Latitude"],
        "CapitalLongitude": ["CapitalLongitude","Longitude","Capital Longitude"],
        "ContinentName": ["ContinentName","Continent"]
    }
    for need, alts in alias.items():
        if need not in caps.columns:
            for a in alts:
                if a in caps.columns:
                    caps.rename(columns={a: need}, inplace=True)
                    break
    caps["_join"] = caps["CountryName"].apply(fix_name_for_join)
    caps = caps[["_join","CountryName","CapitalLatitude","CapitalLongitude","ContinentName"]]
else:
    caps = pd.DataFrame(columns=["_join","CountryName","CapitalLatitude","CapitalLongitude","ContinentName"])

geo = wide_norm.merge(caps, on="_join", how="left")
geo["AbsLat"] = geo["CapitalLatitude"].abs()
geo["ClimateZone"] = geo["AbsLat"].apply(classify_climate)
geo["Continent"] = geo["ContinentName"].fillna("Unknown")

# -----------------------------
# 4) ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤
# -----------------------------
st.sidebar.subheader("ğŸ§­ ì‹œê°í™” ì„¤ì •")
mbti_sel = st.sidebar.selectbox("MBTI ìœ í˜• ì„ íƒ", MBTI_TYPES, index=MBTI_TYPES.index("INFP"))
size_scale = st.sidebar.slider("ë²„ë¸” í¬ê¸° ë°°ìœ¨", 50, 800, 300, step=50)
opacity_fill = st.sidebar.slider("ì§€ë„ ì±„ì›€ ë¶ˆíˆ¬ëª…ë„", 0.2, 1.0, 0.6, step=0.1)

region_filter = st.sidebar.multiselect("ëŒ€ë¥™ í•„í„°", sorted(geo["Continent"].dropna().unique()))
climate_filter = st.sidebar.multiselect("ê¸°í›„ëŒ€ í•„í„°", ["Tropical","Subtropical","Temperate","Polar","Unknown"])

df_view = geo.copy()
if region_filter:
    df_view = df_view[df_view["Continent"].isin(region_filter)]
if climate_filter:
    df_view = df_view[df_view["ClimateZone"].isin(climate_filter)]

# -----------------------------
# 5) ì„¸ê³„ ì§€ë„ (Altair geoshape + ë²„ë¸”)
# -----------------------------
world_layer = alt.Chart(
    alt.topo_feature(WORLD_TOPOJSON_URL, "countries")
).mark_geoshape(
    fill="#e7e7e7", stroke="#aaaaaa", strokeWidth=0.5, opacity=opacity_fill
).properties(width=980, height=520)

points_df = df_view[[country_col,"CapitalLatitude","CapitalLongitude","ClimateZone","Continent", mbti_sel]].copy()
points_df = points_df.rename(columns={
    "CapitalLatitude":"lat",
    "CapitalLongitude":"lon",
    mbti_sel:"value"
})

tooltip = [
    alt.Tooltip(country_col, title="Country"),
    alt.Tooltip("Continent:N", title="Region"),
    alt.Tooltip("ClimateZone:N", title="Climate"),
    alt.Tooltip("value:Q", title=f"{mbti_sel} (%)", format=".2f"),
]

bubble = alt.Chart(points_df.dropna(subset=["lat","lon"])).encode(
    longitude="lon:Q",
    latitude="lat:Q",
    size=alt.Size("value:Q", title=f"{mbti_sel} (%)", scale=alt.Scale(range=[10, size_scale])),
    color=alt.Color("ClimateZone:N", legend=alt.Legend(title="Climate")),
    tooltip=tooltip
).mark_circle(opacity=0.85).properties(width=980, height=520)

map_chart = (world_layer + bubble).project(type="equalEarth").resolve_scale(size="independent")
st.subheader("ğŸŒ ì„¸ê³„ ì§€ë„ â€” ì„ íƒí•œ MBTI ë¹„ìœ¨ ë²„ë¸”(ìˆ˜ë„ ì¢Œí‘œ) + ê¸°í›„ëŒ€ ìƒ‰ìƒ")
st.altair_chart(map_chart, use_container_width=True)

# -----------------------------
# 6) Region/Climate ë¹„êµ
# -----------------------------
st.subheader("ğŸ“Š ëŒ€ë¥™/ê¸°í›„ëŒ€ Ã— MBTI ë¶„í¬ ë¹„êµ")
left, right = st.columns(2)

with left:
    region_bar = alt.Chart(
        long_norm.merge(geo[[country_col,"Continent"]], on=country_col, how="left")
    ).transform_filter(
        alt.FieldOneOfPredicate(field="Continent", oneOf=region_filter) if region_filter else alt.datum.Continent != "___NOFILTER___"
    ).mark_bar().encode(
        x=alt.X("MBTI:N", sort=MBTI_TYPES, title="MBTI"),
        y=alt.Y("mean(Percent):Q", title="í‰ê·  ë¹„ìœ¨(%)"),
        color=alt.Color("Continent:N", title="Region"),
        tooltip=[alt.Tooltip("Continent:N","Region"), alt.Tooltip("MBTI:N"), alt.Tooltip("mean(Percent):Q",format=".2f")]
    ).properties(title="Region(ëŒ€ë¥™)ë³„ MBTI í‰ê· ", width=500, height=320)
    st.altair_chart(region_bar, use_container_width=True)

with right:
    climate_bar = alt.Chart(
        long_norm.merge(geo[[country_col,"ClimateZone"]], on=country_col, how="left")
    ).transform_filter(
        alt.FieldOneOfPredicate(field="ClimateZone", oneOf=climate_filter) if climate_filter else alt.datum.ClimateZone != "___NOFILTER___"
    ).mark_bar().encode(
        x=alt.X("MBTI:N", sort=MBTI_TYPES, title="MBTI"),
        y=alt.Y("mean(Percent):Q", title="í‰ê·  ë¹„ìœ¨(%)"),
        color=alt.Color("ClimateZone:N", title="Climate"),
        tooltip=[alt.Tooltip("ClimateZone:N","Climate"), alt.Tooltip("MBTI:N"), alt.Tooltip("mean(Percent):Q",format=".2f")]
    ).properties(title="Climate(ê°„ì´ ê¸°í›„ëŒ€)ë³„ MBTI í‰ê· ", width=500, height=320)
    st.altair_chart(climate_bar, use_container_width=True)

# -----------------------------
# 7) |ìœ„ë„| ìƒê´€ (Pearson/Spearman)
# -----------------------------
st.subheader("ğŸ“ˆ ìœ„ë„ ì ˆëŒ“ê°’(|lat|)ê³¼ MBTI ë¹„ìœ¨ ìƒê´€")
corr_df = geo[[country_col,"AbsLat"] + MBTI_TYPES].dropna(subset=["AbsLat"]).copy()

pearson = corr_df[MBTI_TYPES].corrwith(corr_df["AbsLat"], method="pearson").rename("Pearson").to_frame()
spearman = corr_df[MBTI_TYPES].corrwith(corr_df["AbsLat"], method="spearman").rename("Spearman").to_frame()
corr_tbl = pearson.join(spearman).reset_index().rename(columns={"index":"MBTI"})

corr_bar = alt.Chart(corr_tbl.melt("MBTI", var_name="Metric", value_name="Corr")).mark_bar().encode(
    x=alt.X("MBTI:N", sort=MBTI_TYPES),
    y=alt.Y("Corr:Q", scale=alt.Scale(domain=[-1,1])),
    color="Metric:N",
    tooltip=[alt.Tooltip("MBTI:N"), alt.Tooltip("Metric:N"), alt.Tooltip("Corr:Q", format=".3f")]
).properties(width=980, height=280)
st.altair_chart(corr_bar, use_container_width=True)

# -----------------------------
# 8) ì„ íƒ ìœ í˜• ìƒìœ„ N êµ­ê°€ í‘œ
# -----------------------------
st.subheader(f"ğŸ… {mbti_sel} ë¹„ìœ¨ ìƒìœ„ êµ­ê°€")
top_n = st.slider("ìƒìœ„ N", 5, 30, 10)
top_tbl = df_view[[country_col,"Continent","ClimateZone", mbti_sel]].rename(columns={mbti_sel:"Percent"}).sort_values("Percent", ascending=False).head(top_n)
st.dataframe(top_tbl.reset_index(drop=True))

# -----------------------------
# 9) ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
# -----------------------------
with st.expander("ì›ë³¸ â†’ ì •ê·œí™” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
    st.write("ì›ë³¸(ìƒìœ„ 5í–‰)")
    st.dataframe(wide.head())
    st.write("ì •ê·œí™”(í•©=100) (ìƒìœ„ 5í–‰)")
    st.dataframe(wide_norm.head())

with st.expander("ìˆ˜ë„/ìœ„ë„/ê¸°í›„ ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
    st.dataframe(geo[[country_col,"Continent","ClimateZone","CapitalLatitude","CapitalLongitude"] + MBTI_TYPES].head())
