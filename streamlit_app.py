# streamlit_app.py
import os
import io
import json
import re
import urllib.request
from typing import Dict, Tuple, Optional

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="MBTI Ã— Climate Map",
    page_icon="ğŸ—ºï¸",
    layout="wide",
)

st.title("ğŸ—ºï¸ MBTI ìœ í˜• Ã— ê¸°í›„(ìœ„ë„) ìƒê´€ ì‹œê°í™”")
st.caption("ê°™ì€ í´ë”ì˜ CSVë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì½ê³ , ì—†ìœ¼ë©´ ì—…ë¡œë“œë¡œ ëŒ€ì²´ Â· Altair ì¸í„°ë™í‹°ë¸Œ ì›”ë“œë§µ Â· ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë¶ˆí•„ìš”")

# -----------------------------
# Constants
# -----------------------------
MBTI_TYPES = [
    "INTJ","INTP","ENTJ","ENTP",
    "INFJ","INFP","ENFJ","ENFP",
    "ISTJ","ISFJ","ESTJ","ESFJ",
    "ISTP","ISFP","ESTP","ESFP"
]
LOCAL_DATA_PATH = "countriesMBTI_16types.csv"

WORLD_TOPOJSON_URL = "https://cdn.jsdelivr.net/npm/world-atlas@2/countries-110m.json"
# Columns: CountryName, CapitalName, CapitalLatitude, CapitalLongitude, ContinentName, ...
CAPITALS_CSV_URL = "https://raw.githubusercontent.com/icyrockcom/country-capitals/master/data/country-list.csv"

NAME_FIX: Dict[str, str] = {
    "united states": "united states of america",
    "usa": "united states of america",
    "u s a": "united states of america",
    "russia": "russian federation",
    "south korea": "korea republic of",
    "north korea": "korea democratic people s republic of",
    "czech": "czechia",
    "czech republic": "czechia",
    "swaziland": "eswatini",
    "cape verde": "cabo verde",
    "laos": "lao people s democratic republic",
    "moldova": "moldova republic of",
    "ivory coast": "cote d ivoire",
    "brunei darussalam": "brunei",
    "vatican": "holy see",
}

# -----------------------------
# Utils
# -----------------------------
def norm_name(name: str) -> str:
    if not isinstance(name, str):
        return ""
    s = name.lower().strip()
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

def fix_name_for_join(s: str) -> str:
    s0 = norm_name(s)
    return NAME_FIX.get(s0, s0)

@st.cache_data(show_spinner=False)
def robust_read_csv(source) -> pd.DataFrame:
    """Try multiple encodings for path/UploadedFile/URL."""
    if isinstance(source, str) and os.path.exists(source):
        return pd.read_csv(source)
    if hasattr(source, "read"):  # Streamlit UploadedFile
        raw = source.read()
        for enc in ("utf-8","utf-8-sig","cp949","euc-kr","latin-1"):
            try:
                return pd.read_csv(io.BytesIO(raw), encoding=enc)
            except Exception:
                pass
        return pd.read_csv(io.BytesIO(raw), engine="python")
    return pd.read_csv(source)

@st.cache_data(show_spinner=False)
def fetch_json(url: str) -> dict:
    with urllib.request.urlopen(url) as resp:
        return json.loads(resp.read().decode("utf-8"))

@st.cache_data(show_spinner=False)
def fetch_csv(url: str) -> pd.DataFrame:
    return pd.read_csv(url)

def auto_detect_country_col(df: pd.DataFrame) -> Optional[str]:
    for cand in ["Country","country","êµ­ê°€","êµ­ê°€ëª…","Nation","Region","Country/Region"]:
        if cand in df.columns:
            return cand
    # fallback: ì²« ë²ˆì§¸ ë¹„ìˆ˜ì¹˜í˜• ì—´
    for c in df.columns:
        if not pd.api.types.is_numeric_dtype(df[c]):
            return c
    return None

def to_wide_by_country(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:
    """
    CSVë¥¼ êµ­ê°€ë³„ ê°€ë¡œí˜• í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ê³ ,
    íƒì§€í•œ êµ­ê°€ ì—´ì„ í•­ìƒ 'Country'ë¡œ ê°•ì œ í†µì¼í•˜ì—¬ ë°˜í™˜í•œë‹¤.
    """
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]

    # 1) êµ­ê°€ ì—´ íƒì§€ + 'Country'ë¡œ ê°•
